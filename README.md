---
title: LeetGPU é¢˜ç›®è§£æ³•è®°å½•
date: 2026-01-07 00:21
modified: 2026-01-07 00:21
tags:
  - cuda
  - leetgpu
  - parallel-computing
  - kernel-optimization
  - practice
categories:
  - å®è·µé¡¹ç›®
excerpt: è®°å½• LeetGPU å¹³å°ä¸Šçš„ CUDA ç¼–ç¨‹é¢˜ç›®ä¸è§£æ³•ï¼Œæ¶µç›– softmaxã€çŸ©é˜µè¿ç®—ç­‰ç»å…¸å¹¶è¡Œè®¡ç®—é—®é¢˜çš„å®ç°ä¸ä¼˜åŒ–ã€‚
mathjax: true
comment: true
---

# LeetGPU é¢˜ç›®è§£æ³•è®°å½•

> é¢˜ç›®å¹³å°ï¼š[LeetGPU](https://leetgpu.com/challenges) - CUDA ç¼–ç¨‹ç®—æ³•ç»ƒä¹ 
>
> æœ¬æ–‡æ¡£è®°å½• LeetGPU å¹³å°ä¸Šçš„å„ç±» CUDA ç¼–ç¨‹é¢˜ç›®ï¼ŒåŒ…æ‹¬é¢˜ç›®åˆ†æã€è§£æ³•å®ç°ã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§å’Œä»£ç è§£æã€‚

## ç›®å½•

- [é¢˜ç›®åˆ—è¡¨](#é¢˜ç›®åˆ—è¡¨)
- [LeetGPU CLI ä½¿ç”¨æŒ‡å—](#leetgpu-cli-ä½¿ç”¨æŒ‡å—)
- [é¢˜ç›® 1: Softmax](#é¢˜ç›®-1-softmax)
- [é¢˜ç›® 2: å¾…æ·»åŠ ](#é¢˜ç›®-2-å¾…æ·»åŠ )
- [é€šç”¨ä¼˜åŒ–æŠ€å·§æ€»ç»“](#é€šç”¨ä¼˜åŒ–æŠ€å·§æ€»ç»“)

---

## LeetGPU CLI ä½¿ç”¨æŒ‡å—

> å®˜æ–¹æ–‡æ¡£ï¼š[LeetGPU CLI](https://leetgpu.com/cli)

LeetGPU æä¾›äº† CLI å·¥å…·ç”¨äºæœ¬åœ°æµ‹è¯•å’ŒéªŒè¯ CUDA ç¨‹åºçš„æ­£ç¡®æ€§ï¼Œæ— éœ€åå¤ä¸Šä¼ åˆ°ç½‘ç«™ã€‚

### å®‰è£…

```bash
# ä½¿ç”¨ npm å®‰è£…
npm install -g leetgpu-cli
```

### åŸºæœ¬ç”¨æ³•

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
leetgpu --help

# æµ‹è¯•æœ¬åœ°è§£å†³æ–¹æ¡ˆ
leetgpu test <problem_name> <solution_file>

# ç¤ºä¾‹ï¼šæµ‹è¯• Softmax é¢˜ç›®
leetgpu test softmax solution.cu

# æŸ¥çœ‹é¢˜ç›®åˆ—è¡¨
leetgpu list

# è·å–é¢˜ç›®æ¨¡æ¿
leetgpu init <problem_name>
```

### å·¥ä½œæµç¨‹

1. **åˆå§‹åŒ–é¢˜ç›®**ï¼š
   ```bash
   leetgpu init softmax
   ```

2. **ç¼–å†™è§£å†³æ–¹æ¡ˆ**ï¼ˆåœ¨ `solution.cu` ä¸­å®ç° `solve()` å‡½æ•°ï¼‰

3. **æœ¬åœ°æµ‹è¯•**ï¼š
   ```bash
   leetgpu test softmax solution.cu
   ```

4. **æŸ¥çœ‹ç»“æœ**ï¼š
   - âœ“ é€šè¿‡ï¼šè¾“å‡ºæ€§èƒ½æŒ‡æ ‡
   - âœ— å¤±è´¥ï¼šæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯å’Œæµ‹è¯•ç”¨ä¾‹

### æ”¯æŒçš„é¢˜ç›®

- `softmax` - Softmax ç®—å­å®ç°
- å…¶ä»–é¢˜ç›®æŒç»­æ›´æ–°ä¸­

### æœ¬åœ°éªŒè¯ç¤ºä¾‹

```bash
# å®Œæ•´ç¤ºä¾‹
$ leetgpu test softmax softmax/solution.cu

Testing Softmax...
  Test case 1: PASS (0.0123ms)
  Test case 2: PASS (0.0234ms)
  Test case 3: PASS (0.0189ms)

All tests passed! âœ“
Performance: 85.6 TFLOPS
```

### ä¼˜åŠ¿

- **å¿«é€Ÿè¿­ä»£**ï¼šæ— éœ€ä¸Šä¼ å³å¯éªŒè¯ä»£ç 
- **è¯¦ç»†åé¦ˆ**ï¼šæ˜¾ç¤ºå…·ä½“é”™è¯¯å’Œæ€§èƒ½æ•°æ®
- **æœ¬åœ°ç¯å¢ƒ**ï¼šæ”¯æŒè‡ªå®šä¹‰ç¼–è¯‘é€‰é¡¹å’Œè°ƒè¯•

---

## é¢˜ç›®åˆ—è¡¨

| # | é¢˜ç›®åç§° | éš¾åº¦ | æ ¸å¿ƒæŠ€å·§ | çŠ¶æ€ |
|---|---------|------|---------|------|
| 1 | [Softmax](#é¢˜ç›®-1-softmax) | Medium | Warp Primitives, Reduction, Numerical Stability | âœ… å®Œæˆ |
| 2 | å¾…æ·»åŠ  | - | - | ğŸš§ TODO |
| 3 | å¾…æ·»åŠ  | - | - | ğŸš§ TODO |

---

## é¢˜ç›® 1: Softmax

> é¢˜ç›®é“¾æ¥ï¼š[LeetGPU - Softmax](https://leetgpu.com/challenges/softmax)
>
> å®ç°é«˜æ•ˆçš„ CUDA Softmax kernelï¼Œå¤„ç†å¤§æ•°ç»„çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚

### é—®é¢˜å®šä¹‰

**è¾“å…¥**ï¼š
- `const float* input`: è¾“å…¥æ•°ç»„ï¼ˆGPU å†…å­˜ï¼‰
- `int N`: æ•°ç»„é•¿åº¦
- `float* output`: è¾“å‡ºæ•°ç»„ï¼ˆGPU å†…å­˜ï¼‰

**è¾“å‡º**ï¼š
- è®¡ç®— softmax å¹¶å†™å…¥ output

**Softmax å…¬å¼**ï¼š
$$ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}} $$

### å…³é”®ç‚¹

1. **æ•°å€¼ç¨³å®šæ€§**ï¼šå¤§æ•°å€¼ç›´æ¥è®¡ç®— $e^x$ ä¼šæº¢å‡º
2. **å¹¶è¡Œè§„çº¦**ï¼šéœ€è¦é«˜æ•ˆçš„æ±‚æœ€å¤§å€¼å’Œæ±‚å’Œç®—æ³•
3. **å†…å­˜å¸¦å®½**ï¼šä¼˜åŒ–è®¿å­˜æ¨¡å¼ï¼Œæé«˜å¸¦å®½åˆ©ç”¨ç‡

### è§£æ³•å®ç°

#### å®Œæ•´ä»£ç 

```cpp
#include <cuda_runtime.h>
#include <float.h>

#define WARP_SIZE 32

// Warp çº§æ±‚å’Œè§„çº¦
__device__ __forceinline__ float warpReduceSum(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

// Warp çº§æ±‚æœ€å¤§å€¼è§„çº¦
__device__ __forceinline__ float warpReduceMax(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}

__global__ void softmax_kernel(const float* input, float* output, int N) {

    // è®¡ç®—æœ€å¤§å€¼
    __shared__ float shared_val[32]; // 256 / 32 = 8ï¼Œæ¯ä¸ªwarpå­˜å‚¨ä¸€ä¸ªæœ€å¤§å€¼

    int tid = threadIdx.x;
    int wid = tid / WARP_SIZE;
    int lane = tid % WARP_SIZE;

    float local_max = -FLT_MAX;

    //å±€éƒ¨æœ€å¤§å€¼
    // Grid-Stride Loopåˆå¹¶è®¿å­˜
    // è®©ç›¸é‚»çš„çº¿ç¨‹è®¿é—®ç›¸é‚»çš„æ•°æ®
    for(int i = tid; i < N;i += blockDim.x){
        local_max = fmaxf(local_max, input[i]);
    }

    // å½“å‰warpçš„æœ€å¤§å€¼
    local_max = warpReduceMax(local_max);
    if(lane == 0){
        shared_val[wid] = local_max;
    }
    __syncthreads();

    int warps = blockDim.x / WARP_SIZE;
    float block_max = (tid < warps) ? shared_val[lane] : -FLT_MAX;
    if(wid == 0){
        block_max = warpReduceMax(block_max);
        shared_val[0] = block_max;
    }
    __syncthreads();
    float final_max = shared_val[0];

    // æ±‚å’Œ
    float local_sum = 0.0f;
    for(int i = tid;i < N;i += blockDim.x){
        local_sum += __expf(input[i] - final_max); // max trick
    }

    local_sum = warpReduceSum(local_sum);
    if(lane == 0)shared_val[wid] = local_sum;

    __syncthreads();

    float block_sum = (tid < warps) ? shared_val[lane] : 0;
    if(wid == 0){
        block_sum = warpReduceSum(block_sum);
        shared_val[0] = block_sum;
    }
    __syncthreads();
    float final_sum = shared_val[0];

    // è®¡ç®—softmax
    for(int i = tid;i < N;i+=blockDim.x){
        output[i] = __expf(input[i] - final_max) / final_sum;
    }
}

// input, output are device pointers (i.e. pointers to memory on the GPU)
extern "C" void solve(const float* input, float* output, int N) {
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;

    softmax_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, output, N);
    cudaDeviceSynchronize();
}
```

### æ ¸å¿ƒæŠ€å·§è§£æ

#### 1. Warp-Level Primitives

ä½¿ç”¨ `__shfl_down_sync()` å®ç° warp å†…çº¿ç¨‹é—´é€šä¿¡ï¼š

```cpp
// Warp çº§è§„çº¦ï¼š5 æ­¥å®Œæˆï¼ˆlog2(32) = 5ï¼‰
for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
    val += __shfl_down_sync(0xffffffff, val, offset);
```

**ä¼˜åŠ¿**ï¼š
- æ— éœ€å…±äº«å†…å­˜
- æ—  `__syncthreads()` å¼€é”€
- å»¶è¿Ÿæä½ï¼ˆ~1-2 cyclesï¼‰

#### 2. Grid-Stride Loop

```cpp
for(int i = tid; i < N; i += blockDim.x) {
    // å¤„ç† input[i]
}
```

**è®¿å­˜ä¼˜åŒ–**ï¼š
- ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»åœ°å€ï¼ˆåˆå¹¶è®¿å­˜ï¼‰
- å‡å°‘å†…å­˜äº‹åŠ¡æ•°é‡
- æé«˜ L2 Cache å‘½ä¸­ç‡

#### 3. ä¸¤çº§è§„çº¦ç­–ç•¥

```
256 Threads = 8 Warps
â”œâ”€ Warp çº§è§„çº¦ï¼ˆå¹¶è¡Œï¼‰
â”‚   Warp 0 â†’ sum0, Warp 1 â†’ sum1, ..., Warp 7 â†’ sum7
â”œâ”€ å†™å…¥å…±äº«å†…å­˜
â”‚   shared[0..7] = {sum0, ..., sum7}
â””â”€ Warp 0 æœ€ç»ˆè§„çº¦
    final_sum = sum0 + ... + sum7
```

**åŒæ­¥æ¬¡æ•°**ï¼šä»… 2 æ¬¡ï¼ˆvs ç›´æ¥è§„çº¦çš„ 8 æ¬¡ï¼‰

#### 4. Max Trick æ•°å€¼ç¨³å®šæ€§

**é—®é¢˜**ï¼š$e^{1000}$ ä¼šæº¢å‡ºä¸º `inf`

**è§£å†³**ï¼š
$$ \text{softmax}(x_i) = \frac{e^{x_i - M}}{\sum_j e^{x_j - M}}, \quad M = \max(\mathbf{x}) $$

å› ä¸º $e^{x_i - M} \leq 1$ï¼Œé¿å…æº¢å‡ºã€‚

### æ€§èƒ½åˆ†æ

#### ç®—æ³•å¤æ‚åº¦

| é˜¶æ®µ | æ“ä½œ | å¤æ‚åº¦ |
|------|------|--------|
| æ±‚æœ€å¤§å€¼ | Grid-Stride + Warpè§„çº¦ + Blockè§„çº¦ | $O(N/256) + O(\log 32) + O(\log 8)$ |
| æ±‚æŒ‡æ•°å’Œ | åŒä¸Š | $O(N/256) + O(\log 32) + O(\log 8)$ |
| è®¡ç®— softmax | Grid-Stride Loop | $O(N/256)$ |

#### å†…å­˜è®¿é—®

```
è¯»å– input: 2 æ¬¡ï¼ˆæ±‚æœ€å¤§å€¼ã€æ±‚å’Œï¼‰
å†™å…¥ output: 1 æ¬¡
æ€»è®¡ï¼š3N æ¬¡å†…å­˜è®¿é—®
```

**ç®—æœ¯å¼ºåº¦**ï¼š$\frac{3N \text{ FLOPs}}{12N \text{ bytes}} \approx 0.25 \text{ FLOP/byte}$ï¼ˆå†…å­˜å¸¦å®½å—é™ï¼‰

### éªŒè¯æµ‹è¯•

```cpp
// æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
float input[] = {1000.0f, 1001.0f, 1002.0f};
solve(d_input, d_output, 3);

// é¢„æœŸè¾“å‡ºï¼š
// output[0] â‰ˆ 0.090
// output[1] â‰ˆ 0.245
// output[2] â‰ˆ 0.665
// sum â‰ˆ 1.0
```

### ä¼˜åŒ–ç©ºé—´

- [ ] ä½¿ç”¨ `__ldg()` åŠ é€Ÿåªè¯»æ•°æ®è®¿é—®
- [ ] Loop unrollingï¼ˆç¼–è¯‘å™¨ `-O3` å¯è‡ªåŠ¨ä¼˜åŒ–ï¼‰
- [ ] å¤„ç†é 2 æ¬¡å¹‚æ•°ç»„å¤§å°
- [ ] Batch Softmaxï¼ˆäºŒç»´è¾“å…¥ï¼‰

---

## é¢˜ç›® 2: å¾…æ·»åŠ 

> é¢˜ç›®é“¾æ¥ï¼š[LeetGPU - ...](https://leetgpu.com/challenges/...)

### é—®é¢˜å®šä¹‰

### è§£æ³•å®ç°

### æ ¸å¿ƒæŠ€å·§è§£æ

---

## é€šç”¨ä¼˜åŒ–æŠ€å·§æ€»ç»“

### 1. å¹¶è¡Œè§„çº¦æ¨¡å¼

#### Warp-Level Reduction

```cpp
// æ±‚å’Œ
__device__ __forceinline__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

// æ±‚æœ€å¤§å€¼
__device__ __forceinline__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset /= 2)
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}
```

#### Block-Level Reductionï¼ˆä¸¤çº§ï¼‰

```cpp
// Step 1: Warp çº§è§„çº¦
float warp_result = warpReduceSum(local_val);

// Step 2: æ¯ä¸ª warp çš„ lane 0 å†™å…¥å…±äº«å†…å­˜
if(lane == 0) shared[wid] = warp_result;
__syncthreads();

// Step 3: ç¬¬ä¸€ä¸ª warp æ±‡æ€»
if(wid == 0) {
    float final = warpReduceSum(shared[lane]);
}
```

### 2. å†…å­˜è®¿é—®ä¼˜åŒ–

#### Grid-Stride Loop

```cpp
// âœ“ å¥½çš„ï¼šåˆå¹¶è®¿å­˜
for(int i = tid; i < N; i += blockDim.x) {
    process(data[i]);
}

// âœ— å·®çš„ï¼šéè¿ç»­è®¿é—®
for(int i = 0; i < N; i += blockDim.x) {
    process(data[tid + i]);
}
```

#### Shared Memory Bank Conflict é¿å…

```cpp
// âœ“ æ— å†²çªï¼ˆ32 åˆ—æˆ– 33 åˆ—ï¼‰
__shared__ float tile[32][32];

// âœ— æœ‰å†²çªï¼ˆå¦‚æ­¥é•¿ä¸º 32ï¼‰
__shared__ float tile[32][32];
float val = tile[row][col * 32];  // 32-way conflict
```

### 3. æ•°å€¼ç¨³å®šæ€§æŠ€å·§

#### Max Trickï¼ˆæŒ‡æ•°å½’ä¸€åŒ–ï¼‰

```cpp
// Softmax, LogSumExp ç­‰
float max_val = reduce_max(input, N);
for(int i = 0; i < N; i++) {
    output[i] = exp(input[i] - max_val);  // é¿å…æº¢å‡º
}
```

#### Kahan Summationï¼ˆé«˜ç²¾åº¦æ±‚å’Œï¼‰

```cpp
float sum = 0.0f, c = 0.0f;
for(int i = 0; i < N; i++) {
    float y = input[i] - c;
    float t = sum + y;
    c = (t - sum) - y;  // è¡¥å¿è¯¯å·®
    sum = t;
}
```

### 4. Occupancy ä¼˜åŒ–

#### èµ„æºä½¿ç”¨æ£€æŸ¥

```cpp
cudaDeviceProp prop;
cudaGetDeviceProperties(&prop, 0);

// è®¡ç®— occupancy
int min_grid_size, block_size;
cudaOccupancyMaxActiveBlocksPerMultiprocessor(
    &min_grid_size, kernel, 256, 0
);
int max_blocks = prop.multiProcessorCount * min_grid_size;
```

#### ä¼˜åŒ–ç­–ç•¥

| èµ„æº | é™åˆ¶ | ä¼˜åŒ–æ–¹æ³• |
|------|------|---------|
| Registers | 64K/block | å‡å°‘å±€éƒ¨å˜é‡ï¼Œä½¿ç”¨ `__restrict` |
| Shared Memory | æ¶æ„ç›¸å…³ | å‡å°‘å…±äº«å†…å­˜ä½¿ç”¨æˆ–å¢åŠ  block å¤§å° |
| Threads | 2048/SM | è°ƒæ•´ block å¤§å°ï¼ˆ128/256/512ï¼‰ |

### 5. æ€§èƒ½åˆ†æå·¥å…·

```bash
# Nsight Compute åˆ†æ
ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum \
    ./your_program

# Nsight Systems åˆ†ææ—¶é—´çº¿
nsys profile --stats=true ./your_program

# nvprof å¿«é€Ÿæ£€æŸ¥
nvprof --metrics shared_load_efficiency ./your_program
```

---

## å‚è€ƒèµ„æº

### CUDA å®˜æ–¹æ–‡æ¡£
- **[CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)** - å®Œæ•´çš„ CUDA ç¼–ç¨‹æŒ‡å—
- **[Warp Shuffle Functions](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions)** - Warp çº§åŸè¯­è¯¦è§£
- **[Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)** - æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### LeetGPU å¹³å°
- **[LeetGPU Home](https://leetgpu.com/)** - CUDA ç¼–ç¨‹ç®—æ³•ç»ƒä¹ å¹³å°
- **[Softmax é¢˜ç›®](https://leetgpu.com/challenges/softmax)** - Softmax é¢˜ç›®é¡µé¢
- **[Leaderboard](https://leetgpu.com/leaderboard)** - æ’è¡Œæ¦œå’Œå­¦ä¹ ä»–äººè§£æ³•

### å­¦ä¹ èµ„æº
- **[NVIDIA Developer Blog](https://developer.nvidia.com/blog/)** - å®˜æ–¹æŠ€æœ¯åšå®¢
- **[CUDA Gems](https://developer.nvidia.com/gpugems/GPUGems/gpugems_preface.html)** - é«˜çº§ GPU ç¼–ç¨‹æŠ€å·§

---

## æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | é¢˜ç›® | æ›´æ–°å†…å®¹ |
|------|------|---------|
| 2026-01-07 | Softmax | æ·»åŠ åˆå§‹è§£æ³•å’ŒæŠ€å·§åˆ†æ |
| - | é¢˜ç›®2 | å¾…æ·»åŠ  |
| - | é¢˜ç›®3 | å¾…æ·»åŠ  |

---

**æ–‡æ¡£ä½œè€…**: Claude Sonnet 4.5
**æœ€åæ›´æ–°**: 2026-01-07
**è®¸å¯**: MIT License
